## Protocol

<!-- From https://f1000research.com/for-authors/article-guidelines/study-protocols

     

Please include a clear rationale for the study, as well as a detailed description of the protocol, including:

    How the sample is to be selected
    Interventions to be measured
    Sample size calculation - i.e. expected number of participants to make the outcome significant
    Primary outcomes to be measured, as well as a list of secondary outcomes
    Data analysis and statistical plan
    Details of any ethical issues relating to the study (and of the ethical approval received).
    Plans for dissemination of the study outcome (including the associated data) once completed.
     

Ethics policies: All research must have been conducted within an appropriate ethical framework. Details of approval by the authors’ institution or an ethics committee must be provided in the Methods section. Please refer to the detailed ’Ethics’ section in our editorial policies for more information.
 -->


### Finding Research Objects

One goal of this work is to determine what kind of artifacts, in practice, can be considered a _research object_. For the purpose of building a corpus we need to have both inclusion and exclusion criteria.

The foundational article on the RO concept is [@doi:10.1016/j.future.2011.08.004] and its workshop predecessor [@doi:10.1109/eScience.2010.21]. The Research Object community has maintained lists of [initiatives](http://www.researchobject.org/initiative/) and [Research Object profiles](http://www.researchobject.org/scopes/) which provide curated, although potentially biased, collections of Research Object approaches and implementations. 

#### Declared Research Object usage

In order to determine potential sources of Research Objects we will start with these community lists, but expand based on a literature review by following any academic citation of the before-mentioned Research Object articles to find potential repositories, tools and communities that may conceptually claim to have or make "research objects". This is a broad interpretation that does not expand into general datasets or packaging formats. The list may be expanded by literate search for "Research Object", the RO vocabularies and standard URLs.

Each of the citing articles will then be assessed to see if they have openly accessible research objects that are possible to identify, and ideally retrieve, by building a programmatic crawler. Ideally such access would use an open harvesting protocol like [OAI-PMH](http://www.openarchives.org/OAI/2.0/openarchivesprotocol.htm) or [ResourceSync](http://www.openarchives.org/rs/1.1/resourcesync), but it is predicted that in the majority of cases custom crawler code will need to be developed per repository, in addition to manual harvesting of identifiers for smaller collections and individual Research Objects. 

#### Keyword searches

In addition to this "self-claimed" research object usage we will search in more general repositories by developing a list of keywords like "research object", "robundle" or the RO vocabulary URLs. We will search in at least:

* <https://github.com/>
* <https://gitlab.com/>
* <http://datacite.org/> 
* <https://zenodo.org/>
* <https://toolbox.google.com/datasetsearch/>
* <https://dataverse.harvard.edu/>

It is predicted that these searches will yield duplicates, but will be used to find potentially new Research Object sources or free-standing instances.

#### Archives with manifests

Finally we will consider broadly Open Data repositories of file archives (e.g. ZIP, tar.gz) to inspect for the presence of a _manifest_-like file (e.g. `/manifest.rdf`). For practical reasons this search will be restricted to a smaller selection of public repositories and formats, e.g. [Zenodo (20k *.zip Datasets)](https://zenodo.org/search?page=1&size=20&q=&file_type=zip&type=dataset), [FigShare ("zip" Datasets)](https://figshare.com/search?q=zip&searchMode=1&types=3), [Mendeley Data "zip" File Set](https://data.mendeley.com/datasets?query=zip&page=0&type=FILE_SET&repositoryType=NON_ARTICLE_BASED_REPOSITORY&source=MENDELEY_DATA).

A list of trigger filename patterns will be developed, including:

* `META-INF/manifest.xml` and `META-INF/container.xml` from [EPUB Open Container Format](https://www.w3.org/publishing/epub3/epub-ocf.html)
* `manifest.xml` from [COMBINE archives](http://co.mbine.org/documents/archive) [@doi:10.1186/s12859-014-0369-z]
* `.ro/manifest.rdf` from [RO Hub](http://www.rohub.org/) [@doi:10.1007/978-3-319-12024-9_9]
* `.ro/manifest.json`  from [Research Object Bundle](https://w3id.org/bundle/) [@doi:10.5281/zenodo.12586] 
* `metadata/manifest.json` from [RO-Bagit](https://w3id.org/ro/bagit) and BDBag [@doi:10.1371/journal.pone.0213013; ]
* `CATALOG.json` from [DataCrate](https://github.com/UTS-eResearch/datacrate) [@doi:10.5281/zenodo.1445817]
* `ro-crate-metadata.jsonld` from [RO-Crate](http://researchobject.org/ro-crate/) [@doi:10.5281/zenodo.3250687]

It is predicted that most of the archive files will _not_ contain such a manifest, therefore they can be inspected "on the fly" by the crawler without intermediate storage, to first detect a short-list of archives that contain a manifest-like file. These can then be downloaded in full for further inspection. File-name matching will inspect potential sub-directories, e.g. to detect `nested/data/manifest.xml`, but will classify these archives differently from direct matches.

### Candidate sources

For each candidate source we will collect and assess:

* Date assessed
* Assessed by
* URL
* Name
* Estimate # ROs
* Estimate # users
* Maintainer/publisher
* Community links  (if any)
* RO profile/format  (if any)
* Identifier scheme(s)  (if any)
* Persistence/Versioning (if any)

Then for each candidate source we will evaluate:

* Accessability - can we retrieve RO and/or their metadata
* License - permissions and/or restrictions to redistribute the ROs and/or their metadata
* Feasibility - can we programmatically retrieve all ROs (or just a sample)?
* Duplication - could the "same" RO be present by multiple identifiers or in other repositories?
* Self-identified - are Research Objects classified as such (or using similar terminology)?

We may contact the provider or maintainer to expand on these questions if unclear from public information, however we are not conducting a formal survey, as our main interest lays in the machine-readable information from the research objects themselves.

We will finally form a shortlist of sources for further harvesting, considering:

* Programmatically access (or interesting enough to warrant manual access)
* Diversity - might this source be different from the majority of sources?
* Legality - are we allowed to retrieve ROs (or their identifiers and metadata?)
* Confidentiality - are the research objects accessible to the public? (anonymous access or access by 'fresh' user registration)


### Handling personally identifiable information

Research Objects may, by their nature, contain information about people and their research activities. It is therefore important that our data collection, processing and potential re-distribution is in consistent with the [General Data Protection Regulation (GDPR)](https://www.gov.uk/government/publications/guide-to-the-general-data-protection-regulation). To this end we will evaluate:

* Does the source have a GDPR-compliant privacy policy or equivalent?
* Is personally identifiable information contained by identifier (e.g. username)?
* May personally identifiable information be contained by the Research Object manifest/description
* May personally identifiable information be contained by the Research Object files/content?
* Does the RO (or the metadata) have a license that permits redistribution and attribution, e.g. [Creative Commons Attribution 4.0 (CC-BY)](https://creativecommons.org/licenses/by/4.0/)?

Evaluating this may require retrieving research objects in the first place, but particular care will be taken to classify Research Objects and their sources according to the above evaluation in order to filter information that can progress to be part of the Open Data RO-Index corpus. This forms a staged inclusion list:

1. Unfiltered list of identifiers for a source will be shared if the identifiers tend not to include personally identifiable information
2. Metadata will be shared if it is accessible and does not tend to include personally identifiable information
3. Metadata and identifier will be shared if an open attribution-permitting license is indicated (or implied by site)
4. Content/files will be shared if accessible and an open license is indicated (or, for archives, implied by archive license)

_Note: In the above, "tend to" will be determined manually by inspecting a smaller subset of typically 10 research objects. The selection will aim to approximate a simple random subset, but may need to be expanded to take into account the overall diversity of ROs at the source, e.g. date, authors, subsystem, formats. The identifiers of the ROs of this subset will be recorded, along with a description of how the subset was selected._

The inclusion list may be further restricted based on findings from further processing (e.g. a repository is found to distribute sensitive data).

It is worth noting that compliance with open licenses like [Creative Commons Attribution 4.0 (CC-BY)](https://creativecommons.org/licenses/by/4.0/) or [Apache License 2.0](https://www.apache.org/licenses/LICENSE-2.0)  **require** attribution to be propagated (if present). Attribution may sometimes take the form of a URL, identifier, project or organization which do not directly identify a person.

The inclusion list will form different subsets of Research Objects:

1. Identified Research Objects
2. "Non-sensitive" (but potentially closed) metadata
3. Open metadata (potentially personally identifiable)
4. Open content (potentially personally identifiable)

Data for any excluded Research Objects will only be kept for the purpose and duration of this study on computer infrastructure managed by The University of Manchester. Data from excluded Research Objects will only be used for non-person-identifiable aggregated results (e.g. number of CSV files) and broad categorization (e.g. vocabularies used in metadata).

The identifiers from category 1, metadata from category 3 and data from category 4 will be shared in the public Open Data repository  [Zenodo](https://zenodo.org/) according to [Zenodo's policies](https://about.zenodo.org/policies/). Metadata from category 3 and 4 above may be exposed for programmatic querying (e.g. SPARQL) or converted to other formats. No additional linking with internal and external data sources will be performed, although the collected Research Objects may already contain such links (e.g. <https://orcid.org/> identifiers of authors); an exception to this rule is that linking will be permitted to detect duplicate Research Objects across multiple sources, and to access resources clearly _aggregated_ as part of the Research Object.

For GDPR purposes the _Data Controller_ is The University of Manchester, data subjects may contact `info@esciencelab.org.uk` for any enquiries, such as to request access to data about themselves, or to request update or removal of personally identifiable information.

### Pre-identified data sources

#### Proto-research objects

* [myExperiment packs](https://www.myexperiment.org/packs)
* [COMBINE archives](https://combinearchive.org/index/) [@doi:10.1186/s12859-014-0369-z; @doi:10.5281/zenodo.10439]
* VoID datasets http://www.openphacts.org/specs/2013/WD-datadesc-20130912/ [@doi:10.1186/s12859-014-0369-z; @doi:10.3233/SW-2012-0088]
* DataONE Data packages [@doi:10.1109/eScience.2018.00019]
* DataLad <https://www.datalad.org/datasets.html> [@doi:10.1145/2896825.2896830]
* [BloodHound](http://bloodhound-tracker.net/) / [Global Biodiversity Information Facility](https://gbif.org) GBIF Darwin Core Archives [@doi:10.1371/journal.pone.0102623] e.g. [@doi:10.5281/zenodo.3405730] <https://www.gbif.org/dataset/search>

#### ORE-based research objects

* CWL Viewer <https://view.commonwl.org/workflows> [@doi:10.7490/f1000research.1114375.1]
* RO Bundle <https://w3id.org/bundle/2014-11-05/> [@doi:10.5281/zenodo.12586] 
* Workflow PROV corpus [@doi:10.1145/2457317.2457376]
* CWLProv 10.1093/gigascience/giz095 aka [@doi:10.5281/zenodo.1208477]
* <http://www.rohub.org/> [@doi:10.1007/978-3-319-12024-9_9]
* <http://rohub.linkeddata.es/>
* SEEK: <https://fairdomhub.org/investigations>
* BDBags with [MinID](http://minid.bd2k.org/) [@doi:10.1371/journal.pone.0213013; ]
* Zenodo e.g. [@doi:10.5281/zenodo.1465897]
* Mendeley Data eg [@doi:10.17632/6wtpgr3kbj.1]
* Maven <https://repository.mygrid.org.uk/artifactory/ops/org/openphacts/data/>
* DocumentObject <https://github.com/binfalse/DocumentObjectCompiler/>
* GitHub search
* EOSC-Life (too early?)

### Software/container-based research objects

* <https://sci-f.github.io/> [@doi:10.1093/gigascience/giy023]
* <https://frictionlessdata.io/specs/data-package/>
* The Journal of Research Objects <http://jro.world/> (see also [presentation](https://docs.google.com/presentation/d/1c4eSbTbaJ2ydEujvL1AA1rWghVFesy__nEtRIZeI-kA/edit?usp=sharing)
* Tonkaz <https://docs.google.com/presentation/d/1YRKCM1KwHyNOz7B6AP2j5qIHjl7Ew9CnAtvXA1xSeNY/edit#slide=id.g419a3a9d09_1_93>
* [ActivePaper](https://www.activepapers.org) / HDF5  [@doi:10.12688/f1000research.5773.3]
  > HDF5 dataset attributes are used to store metadata, including a dataflow graph that records provenance (requirement 5), but also creation time stamps and a data type indicator distinguishing references and executable code from “plain” datasets.

#### 2nd generation ROs

* DataCrate: <https://github.com/UTS-eResearch/datacrate/blob/master/spec/1.0/data_crate_specification_v1.0.md#examples>
* RO-Crate: <https://data.research.uts.edu.au/examples/ro-crate/0.2/>

### Manifest formats

A key characteristic of a Research Object is the presence of a _manifest_ that describes and relates the content. However, multiple potential formats and conventions have emerged for how to serialize such a format. (..)

### Proposed data gathering workflow

The overall data gathering workflow is envisioned as:

1. Traverse repository (or one of its sub-sections) using API like [OAI-PMH](http://www.openarchives.org/OAI/2.0/openarchivesprotocol.htm)
2. Filter for entries that have an archive-like file type (e.g. ZIP, tar.gz)
3. Retrieve entry's Datacite-like metadata from repository (e.g. DOI, author, license)
3. Start downloading archive
4. Stream archive though a utility like [sunzip](https://github.com/madler/sunzip) to list filenames within
5. Record filenames mapped to identifier
6. Select entries which have a manifest-like file in list
7. Re-download selected archives
8. Extract manifest(s) from archives
9. Classify manifests based on format and vocabulary (e.g. RDF/XML using ORE-OAI)
10. Record provenance of data gathering

Post-processing workflow:

1. Convert manifests to a unified RDF format (e.g. N-Triples)
2. Populate quad store (e.g. Apache Jena) with converted manifests
3. 


https://zenodo.org/communities/ro/?page=1&size=20

https://developers.zenodo.org/#metadata-formats

#### Prototype workflow

A [prototype workflow](https://github.com/stain/ro-index-paper/blob/master/code/data-gathering/workflows/zenodo-zip-content.cwl) is being developed using [Common Workflow Language](https://www.commonwl.org/) [@doi:10.6084/m9.figshare.3115156.v2], figure @fig:square-image shows how a a community sub-section of the [Zenodo](https://zenodo.org/) repository is being inspected to list the filenames contained within its downloadable ZIP files.

![
**CWL workflow: List ZIP content for Zenodo community**
Visualization by CWL Viewer <https://w3id.org/cwl/view/git/4360a062e7cff5aadacbf401e8e743a660657680/code/data-gathering/workflows/zenodo-zip-content.cwl>
](https://w3id.org/cwl/view/git/4360a062e7cff5aadacbf401e8e743a660657680/code/data-gathering/workflows/zenodo-zip-content.cwl?format=svg "Inspect downloadable zip files in Zenodo"){#fig:square-image}

In brief the prototype workflow consists of these steps:

1. For a given Zenodo community, e.g. <https://zenodo.org/communities/ro>, retrieve its OAI-PMH entries using DataCite v4 XML e.g. <https://zenodo.org/oai2d?verb=ListRecords&set=user-ro&metadataPrefix=datacite4>
2. Extract the URI for the Zenodo record, e.g. <oai:zenodo.org:1484341>
3. Search-replace to build the URI of the the corresponding Zenodo landing page, e.g. <https://zenodo.org/record/2838898>
4. Retrieve the HTML of the landing page
5. Extract the [link relations](https://tools.ietf.org/html/rfc8288) of type [alternate](https://html.spec.whatwg.org/multipage/links.html#rel-alternate) to find the download links
6. Filter for download links that end with `*.zip`
7. Retrieve ZIP file
8. List filenames within ZIP file
9. Detect filenames like "manifest.rdf" within list and return original Zenodo URI or `null` _(in development)_

The workflow and its components have been tested with the reference implementation `cwltool` [@doi:10.5281/zenodo.3369238] which can provide rich provenance captured in CWLProv research objects [@doi:10.5281/zenodo.3196309] <!-- in press 10.1093/gigascience/giz095-->

In developing this prototype several challenges where immediately detected:

* The [OAI-PMH records](https://zenodo.org/oai2d?verb=ListRecords&set=user-ro&metadataPrefix=datacite4) contain substansive DataCite information, but do not include the links to the Zenodo record or the ZIP downloads. The identifiers for records is in a form that is specific to OAI-PMH, e.g. `oai:zenodo.org:1310621` and had to be rewritten to a URI using the same record number.
  * The [Zenodo API](https://developers.zenodo.org/) does include these additional URIs - but is not available for anonymous use as it requires a registration token. Distributing this token as part of the workflow provenance would give access to the author's Zenodo account.
* The HTML landing page provides links to structured metadata, e.g. as JSON-LD using Schema.org <https://zenodo.org/record/1310621/export/schemaorg_jsonld> but the structured data is included in a `<pre>` block within HTML (for copy-pasting) and not programmatically accessible. HTTP content-negotiation does not give direct access to the structured data.  There are no `alternate` links from the HTML landing page to these "export" pages, so their URIs would have to be manually constructed. 
  * The JSON-LD does include links to downloads of ZIP files etc, but only if the Record is of type _Dataset_ in Zenodo, which [in JSON-LD](https://zenodo.org/record/2838898/export/schemaorg_jsonld#.XXdwzffTU5k) is represented as a [DataDownload](http://schema.org/DataDownload) with a [contentUrl](http://schema.org/contentUrl)
* To list filename contained in a ZIP file, the whole file must be downloaded because in the [ZIP format](https://pkware.cachefly.net/webdocs/casestudies/APPNOTE.TXT) the table of content is written at the end of the file
  * The host providing Zenodo downloads do not support the [HTTP Range](https://tools.ietf.org/html/rfc7233) request header, so it is not possible to download say only the last 128 kB of the ZIP file.
  * Some of the dataset downloads are larger than 1 GB
  * `cwltool` saves all intermediate values until the end of the workflow
* To preserve disk space use during download of ZIP file only, an initial idea was explored of using the CWL [streamable](https://www.commonwl.org/v1.1/CommandLineTool.html#CommandOutputRecordField) feature so that the output of `curl` could be passed to the utillity [sunzip](https://github.com/madler/sunzip) which can extract/inspect ZIP files in a streaming fashion
  * It was found that while the sunzip tool supports streamable extraction and testing based on the intermediate file entry records of the ZIP file, it does not decide or list the filenames before the end of the download. Thus the output of `sunzip -t` (test extract) itself is not streamable
  * An experimental new feature `sunzip -l` was [attempted](https://github.com/madler/sunzip/pull/3), where file names of records are printed as they are encountered. This is at the risk of printing files that have subsequently been deleted from the ZIP file and do not appear in the table of content. This was considered a small risk as the primary purpose was to detect ZIP files containing "manifest-like" files and it was assumed that most deposited ZIP files had been created in a one-off operation and would not have inconsistent file records.
  * The experimental feature however detected that several files found in Zenodo are written with a "deferred length" - the intermitting ZIP records do not record their length and so `sunzip` is unable to find the offset to the next record without actually decompressing the stream.
  * It was found that `cwltool --parallel`  did not seem to start the subsequent step and thus did not facilitate the `streamable` feature. The ZIP files were still saved to disk, and all the ZIP file downloads where completed before any of the ZIP extraction was started.  Future work will explore using the implementation `toil` which have been argued to have better support for concurrency.

<!--
TODO: 

Create PROV entities for each record, download, manifest, datacite etc.

Can we get data/stats for all of Zenodo? Start download to fill .cache

-->

